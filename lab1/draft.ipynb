{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, pickle, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# auto. choose CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "\n",
    "def unpickle_cifar(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    return data_dict\n",
    "\n",
    "# Function to load CIFAR-10 dataset\n",
    "DATA_PATH = '/home/yw9023/deeplearning/project1/cifar-10-python/cifar-10-batches-py'\n",
    "\n",
    "# Load metadata (labels)\n",
    "meta = unpickle_cifar(os.path.join(DATA_PATH, 'batches.meta'))\n",
    "classes = [c.decode('utf-8') for c in meta[b'label_names']]\n",
    "\n",
    "# Load training data\n",
    "train_imgs = []\n",
    "train_targets = []\n",
    "for i in range(1, 6):\n",
    "    batch = unpickle_cifar(os.path.join(DATA_PATH, f'data_batch_{i}'))\n",
    "    train_imgs.append(batch[b'data'])\n",
    "    train_targets += batch[b'labels']\n",
    "\n",
    "train_imgs = np.vstack(train_imgs).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "train_targets = np.array(train_targets)\n",
    "\n",
    "# Data augmentation and normalization\n",
    "train_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomAdjustSharpness(sharpness_factor=2, p=0.2),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    T.RandomErasing(p=0.2, scale=(0.02, 0.1), value=1.0)\n",
    "])\n",
    "\n",
    "# Data augmentation and normalization\n",
    "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.data = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Convert to TensorDataset and apply transformations\n",
    "train_dataset = CIFAR10Dataset(train_imgs, train_targets, transform=train_transform)\n",
    "\n",
    "# Load test batch\n",
    "test_batch = unpickle_cifar(os.path.join(DATA_PATH, 'test_batch'))\n",
    "val_imgs = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "val_targets = np.array(test_batch[b'labels'])\n",
    "val_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "val_dataset = CIFAR10Dataset(val_imgs, val_targets, transform=val_transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load test data\n",
    "TEST_PATH = '/home/yw9023/deeplearning/project1/cifar_test_nolabel.pkl'\n",
    "test_batch_custom = unpickle_cifar(TEST_PATH)\n",
    "test_imgs = test_batch_custom[b'data'].astype(np.float32) / 255.0\n",
    "test_dataset = [(val_transform(img),) for img in test_imgs]\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define training function\n",
    "def fit_model(model, train_dl, valid_dl, n_epochs=50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80, 90], gamma=0.1)\n",
    "    stats = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss, correct_preds, total_samples = 0.0, 0, 0\n",
    "        for imgs, labels in train_dl:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_dl)\n",
    "        train_accuracy = 100.0 * correct_preds / total_samples\n",
    "        stats['train_loss'].append(avg_train_loss)\n",
    "        stats['train_acc'].append(train_accuracy)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in valid_dl:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "        avg_val_loss = val_loss / len(valid_dl)\n",
    "        val_accuracy = 100.0 * val_correct / val_total\n",
    "        stats['val_loss'].append(avg_val_loss)\n",
    "        stats['val_acc'].append(val_accuracy)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f}, Train Acc {train_accuracy:.2f}%, \"\n",
    "              f\"Val Loss {avg_val_loss:.4f}, Val Acc {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Plot Losses and accuracies\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(1, n_epochs+1), stats['train_loss'], 'r-', label='Train Loss')\n",
    "    plt.plot(range(1, n_epochs+1), stats['val_loss'], 'b-', label='Val Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(1, n_epochs+1), stats['train_acc'], 'g-', label='Train Accuracy')\n",
    "    plt.plot(range(1, n_epochs+1), stats['val_acc'], 'm-', label='Val Accuracy')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define a custom ResNet model from scratch\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.act_fn = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act_fn(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += shortcut\n",
    "        return self.act_fn(out)\n",
    "\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyResNet, self).__init__()\n",
    "        self.stem_conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.stem_bn = nn.BatchNorm2d(64)\n",
    "        self.stem_act = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._build_layer(64, 64, num_blocks=4, stride=1)\n",
    "        self.layer2 = self._build_layer(64, 128, num_blocks=4, stride=2)\n",
    "        self.layer3 = self._build_layer(128, 256, num_blocks=3, stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    def _build_layer(self, in_ch, out_ch, num_blocks, stride):\n",
    "        layers = [ResBlock(in_ch, out_ch, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResBlock(out_ch, out_ch))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.stem_bn(x)\n",
    "        x = self.stem_act(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Print the number of parameters\n",
    "model_instance = MyResNet(num_classes=10).to(device)\n",
    "from torchsummary import summary\n",
    "summary(model_instance, (3, 32, 32))\n",
    "\n",
    "# Train the model\n",
    "fit_model(model_instance, train_loader, val_loader, n_epochs=100)\n",
    "\n",
    "# Generate submission file\n",
    "model_instance.eval()\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        imgs = batch[0].to(device)\n",
    "        outputs = model_instance(imgs)\n",
    "        _, pred = outputs.max(1)\n",
    "        preds_list.extend(pred.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'ID': np.arange(len(preds_list)), 'Labels': preds_list})\n",
    "submission.to_csv('/kaggle/working/submission1.csv', index=False)\n",
    "print(\"Submission file saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
